{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizadores, Learning Rate Schedulers\n",
    "\n",
    "En esta sesión vamos a comparar el resultado diferentes optimizadores y learning rates schedulers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer, normalizar y particionar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importar y normalizar datos\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('training set', x_train.shape)\n",
    "print('test set', x_test.shape)\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize [0..255]-->[0..1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes=10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print('training set', x_train.shape)\n",
    "print('val set', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizadores\n",
    "\n",
    "Vamos a probar diferentes optimizadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD,Adam,Adagrad\n",
    "\n",
    "batch_size=128\n",
    "epochs=5\n",
    "lr=0.001\n",
    "\n",
    "opt=[]\n",
    "opt.append(SGD(learning_rate=lr, momentum=0.9))\n",
    "opt.append(Adam(learning_rate=lr))\n",
    "opt.append(Adagrad(learning_rate=lr))\n",
    "\n",
    "best_acc=0.0\n",
    "for optim in opt:\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(784))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    " \n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val)) ## <--- OJO validation set\n",
    "    \n",
    "    print(\"\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
    "    \n",
    "    if history.history['val_accuracy'][-1]>best_acc:\n",
    "        best_acc=history.history['val_accuracy'][-1]\n",
    "        bestopt=optim\n",
    "\n",
    "print(\"=============================\")\n",
    "print(\"Best acc\",best_acc)\n",
    "print(\"Best optim\",bestopt)\n",
    "print(\"=============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EJERCICIO**\n",
    "\n",
    "Añade más epochs a este ejemplo anterior y un early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rates Schedulers\n",
    "\n",
    "Los learning rate schedulers son mecanismos de modificación del learning rate. Normalmente lo que hacen es reducir el valor del learning rate, lo que se conoce como ***learning rate annealing***. Esta modificación se suele realizar al acabar cada epoch.\n",
    "\n",
    "Keras ya dispone de algunos learning rate schedulers implementados pero el usuario puede implemetar su propia estrategia de annealing. Veamos ambos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRS ya implementado en Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emplear un LRS estandard de Keras: ReduceLROnPlateau\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                patience=2, min_lr=0.00001)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[reduce_lr])  ## <--- aquí está\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRS propio mediante función \n",
    "\n",
    "La función toma como entrada el epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emplear un LRS propio: LearningRateScheduler\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "LRS=LearningRateScheduler(scheduler, verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])   \n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[LRS])\n",
    "\n",
    " \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRS propio tipo CosineAnnealing\n",
    "\n",
    "![cosine annealing](cosine.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emplear un LRS propio, CosineAnnealingScheduler\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "lr_max=0.001\n",
    "lr_min=0.00001\n",
    "epochs=100\n",
    "def cosine_annealing(x): # recordemos, x es el número de epoch\n",
    "    lr = lr_max/2 * (1 + math.cos(math.pi * x / epochs))\n",
    "    if lr<lr_min:\n",
    "        lr=lr_min\n",
    "    return lr\n",
    "\n",
    "LRS = LearningRateScheduler(cosine_annealing)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[LRS])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRS propio tipo CosineAnnealing with restarts\n",
    "\n",
    "![cosine annealing restarts](cosinerestart.png)\n",
    "\n",
    "Este scheduler tiene sentido cuanto queremos guardarnos cada uno de los estado del modelo alcanzado en el mínimo LR para luego combinarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emplear un LRS propio, CosineAnnealingScheduler with restarts\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "lr_max=0.001\n",
    "lr_min=0.00001\n",
    "epochs=100\n",
    "\n",
    "def cosine_annealing_with_restarts(x):\n",
    "    lr = lr_max/2 * (1 + math.cos(math.pi * (x % (epochs/5)) / (epochs/5)))\n",
    "    if lr<lr_min:\n",
    "        lr=lr_min\n",
    "    return lr\n",
    "\n",
    "LRS = LearningRateScheduler(cosine_annealing_with_restarts)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(784))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[LRS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA, estandaricemos la clase MLP\n",
    "\n",
    "Vamos a implementar una clase para ahorrarnos escribir código recurrentemente. \n",
    "Para ello empleamos la **functional api** de Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "\n",
    "class MLP(keras.Model):\n",
    "\n",
    "  def __init__(self, input_size,num_classes,hidden=[128]):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.hidden = []\n",
    "    self.num_h=len(hidden)\n",
    "\n",
    "    for h in hidden:\n",
    "       self.hidden.append(Dense(h, activation='relu'))\n",
    "    self.out = Dense(num_classes, activation='softmax')\n",
    "    \n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    \n",
    "    x=self.hidden[0](inputs)\n",
    "    for h in range(1,self.num_h):\n",
    "        x = self.hidden[h](x)\n",
    "    x = self.out(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "model = MLP(784,10,[1024,512])  ## <-- aquí se instancia el modelo, input, num clases y lista con hidden layers\n",
    "model.build((None,784)) ## Esto es necesario para poder instaciar adecuadamente todos los shapes del grafo de computación\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
