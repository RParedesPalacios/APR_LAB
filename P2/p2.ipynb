{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variaciones sobre el modelo inicial \n",
    "\n",
    "Vamos a implementar alguna modificación sobre el modelo inicial propuesto en la primera sesión.\n",
    "\n",
    "Para empezar vamos a definir un conjunto de validación. Con este conjunto de calidación extraído del propio conjunto de entrenamiento estimaremos los mejores parámetros. Dejaremos el conjunto de test sólo para estimar el acierto final con estos mejores parámetros.\n",
    "\n",
    "Primero leemos los datos y normalizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('training set', x_train.shape)\n",
    "print('test set', x_test.shape)\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize [0..255]-->[0..1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes=10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partición training/validación\n",
    "\n",
    "Nos quedaremos con un 80% de los datos para entrenar y un 20% para validar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print('training set', x_train.shape)\n",
    "print('val set', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentación con conjunto de validación\n",
    "\n",
    "Cualquier parámetro a modificar en nuestro modelo debería ser probado sobre el conjunto de validación y escoger aquel que produjera el mejor resultado para probar ese (y sólo ese) sobre el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "batch_size=128\n",
    "epochs=5  \n",
    "\n",
    "hdim=[128,256,512,1024]\n",
    "best_acc=0.0\n",
    "for h in hdim:\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(784))\n",
    "    model.add(Dense(h, activation='relu')) # <-- repetir esta línea tantas veces como número de capas ocultas se quiera\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    sgd=SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    " \n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val)) ## <--- OJO validation set\n",
    "    \n",
    "    if history.history['val_accuracy'][-1]>best_acc:\n",
    "        best_acc=history.history['val_accuracy'][-1]\n",
    "        besth=h\n",
    "\n",
    "print(\"=============================\")\n",
    "print(\"Best acc\",best_acc)\n",
    "print(\"Best hidden dim\",besth)\n",
    "print(\"=============================\")\n",
    "\n",
    "\n",
    "## PROBAR EL MEJOR CON TEST\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "model.add(Dense(besth, activation='relu'))  \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "sgd=SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, # <--- aquí podríamos concatenar trainin+valid para no malgastar datos\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EARLY STOPPING\n",
    "\n",
    "De la multitud de parámetros a decidir en la definición y entrenamiento de las redes neuronales hay uno que podríamos automatizar. Es el número de epochs a emplear. Dado que tenemos un conjunto de validación vamos a emplearlo para monitorizar cómo evoluciona la convergencia. Si en algún momento el descenso por gradiente parece haber alcanzado una zona de meseta sobre dicho conjunto de validación podremos detener el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "sgd=SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3) ## También podría ser 'loss' sin necesitar un conjunto de validación\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs=100  ## No nos preocupemos, el fit acabará antes por el early_stopping\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[callback])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio**:\n",
    "\n",
    "Entrenar una red neuronal MLP para el problema MNIST. Los valores que se quieren probar son los siguientes:\n",
    "\n",
    "-   Número de capas ocultas [1,2,3]\n",
    "-   Dimensión de las capas ocultas [512,1024]\n",
    "-   Learning rate [0.01, 0.001, 0.0001]\n",
    "-   Batch_size [128]\n",
    "-   Epochs con early_stopping\n",
    "\n",
    "Obtener la mejor combinación con el conjunto de validación y probarla finalmente sobre el conjunto de test"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
