{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio**:\n",
    "\n",
    "Entrenar una red neuronal MLP para el problema MNIST. Los valores que se quieren probar son los siguientes:\n",
    "\n",
    "-   Número de capas ocultas [1,2,3]\n",
    "-   Dimensión de las capas ocultas [512,1024]\n",
    "-   Learning rate [0.01, 0.001]\n",
    "-   Batch_size [128]\n",
    "-   Epochs con early_stopping\n",
    "\n",
    "Obtener la mejor combinación con el conjunto de validación y probarla finalmente sobre el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlayers=[1,2,3]\n",
    "vdim=[512,1024]\n",
    "vlr=[0.025,0.01]\n",
    "batch_size=128\n",
    "\n",
    "best=0.0\n",
    "for layers in vlayers:\n",
    "    for dim in vdim:\n",
    "        for lr in vlr:\n",
    "            print(\"===========================\")\n",
    "            print(\"Num hidden layers=\",layers)\n",
    "            print(\"Dim hidden layers=\",dim)\n",
    "            print(\"Learning rate=\",lr)\n",
    "            print(\"===========================\")\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Input(784))\n",
    "            for i in range(layers):\n",
    "                model.add(Dense(dim, activation='relu'))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "            sgd=SGD(learning_rate=lr, momentum=0.9)\n",
    "\n",
    "            callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3) ## También podría ser 'loss' sin necesitar un conjunto de validación\n",
    "\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=sgd,\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            epochs=100  ## No nos preocupemos, el fit acabará antes por el early_stopping\n",
    "            history = model.fit(x_train, y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs,\n",
    "                                verbose=1,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[callback])\n",
    "            res=max(history.history['val_accuracy'][:]) ## <--- el mejor, no el último, luego aprenderemos a guardarnos este mejor y no quedarnos con el último modelo\n",
    "            print(\"\\n RES=\",res,\"\\n\")\n",
    "            if res>best:\n",
    "                best=res\n",
    "                print(\"***********************\")\n",
    "                best_dim=dim\n",
    "                best_layers=layers\n",
    "                best_lr=lr\n",
    "\n",
    "print(\"===========================\")\n",
    "print(\"Best num hidden layers=\",best_layers)\n",
    "print(\"Best dim hidden layers=\",best_dim)\n",
    "print(\"Best learning rate=\",best_lr)\n",
    "print(\"===========================\")\n",
    "\n",
    "## PROBAR EL MEJOR CON TEST\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "for i in range(best_layers):\n",
    "    model.add(Dense(best_dim, activation='relu'))  \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "sgd=SGD(learning_rate=best_lr, momentum=0.9)\n",
    "\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3) ## También podría ser 'loss' sin necesitar un conjunto de validación\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs=100  ## No nos preocupemos, el fit acabará antes por el early_stopping\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustar parámetros\n",
    "\n",
    "Es posible que en el ejemplo anterior el entrenamiento haya acabado prematuramente por el efecto del Early Stopping. En ese caso, el modelo que se ha obtenido no es el mejor posible. Para obtener el mejor modelo quizás sea necesario aumentar el número de epochs y para ello modificar algún parámetro del early stopping.\n",
    "\n",
    "## Ejercicio: \n",
    "\n",
    "Modificar el early stopping y emplear un modelo con los mejores parámetros (número de capas ocultas, learning rate, batch size, etc) para evaluar finalmente el modelo guardado sobre el test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solución \n",
    "\n",
    "# Cada uno que pruebe con la mejor topología, en mi caso 3 capas ocultas de 1024 neuronas cada una \n",
    "# y learning rate 0.025\n",
    "# Pero lo importante es que reduzcan el min_delta a 0.001 o incluso menos, en mi caso 0.0001\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(784))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "sgd=SGD(learning_rate=0.025, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    " \n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3) \n",
    "\n",
    "# Añade un model checkpoint para guardar el mejor modelo\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True,verbose=1)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[callback, checkpoint])s\n",
    "\n",
    "## Cargar el mejor modelo y evaluarlo con el test set\n",
    "model = keras.models.load_model('best_model.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
